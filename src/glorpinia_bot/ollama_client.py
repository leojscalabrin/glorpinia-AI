import os
import requests
import re
import logging

# Imports para compatibilidade com o MemoryManager (LangChain/SQLite/FAISS)
try:
    from langchain.schema import HumanMessage, AIMessage
except ImportError:
    # Fallback se a LangChain n칚o estiver instalada
    class HumanMessage:
        def __init__(self, content): self.content = content
    class AIMessage:
        def __init__(self, content): self.content = content

try:
    from .memory_manager import MemoryManager
except ImportError:
    logging.warning("O MemoryManager n칚o foi encontrado. O bot n칚o ter치 mem칩ria de longo prazo.")
    # Se for estritamente necess치rio um placeholder para rodar
    class MemoryManager:
        def load_user_memory(self, *args): pass
        def save_user_memory(self, *args): pass
        @property
        def vectorstore(self): return None


# Configura칞칚o da URL base do Ollama, lendo de vari치veis de ambiente
OLLAMA_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL_NAME", "glorpinia")

class OllamaClient:
    """
    Cliente para interagir com o modelo Glorpinia customizado no servidor Ollama.
    """
    def __init__(self, personality_profile):
        self.personality_profile = personality_profile
        self.memory = None 

    def get_response(self, query, channel, author, memory_mgr: 'MemoryManager'):
        """
        Gera uma resposta usando a API de chat do Ollama, injetando RAG como contexto.
        """
        if not OLLAMA_MODEL or not OLLAMA_URL:
            logging.error("Vari치veis de ambiente OLLAMA_MODEL_NAME ou OLLAMA_API_URL n칚o definidas.")
            return f"@{author}, Erro de configura칞칚o glorp O portal de Ollama est치 offline. RIPBOZO"

        memory_mgr.load_user_memory(channel, author)
        vectorstore = memory_mgr.vectorstore
        
        # Recupera칞칚o de Contexto (RAG/Mem칩ria de Longo Prazo)
        long_term_context = ""
        if vectorstore:
            try:
                # O retriever busca intera칞칫es passadas relevantes
                retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
                docs = retriever.invoke(query)
                
                if docs:
                    # Formata as intera칞칫es passadas em um bloco de CONTEXTO APRENDIDO
                    long_term_context = "\n".join([doc.page_content for doc in docs])
                    long_term_context = f"**CONTEXTO APRENDIDO (MEM칍RIA GLORPINIA):** {long_term_context}"
            except Exception as e:
                logging.error(f"[RAG ERROR] Falha ao buscar contexto: {e}")

        # Montagem do System Prompt (Refor칞o do RAG)
        system_prompt = f'''
        Voc칡 칠 Glorpinia. Sua personalidade est치 definida no seu sistema.
        Use o seguinte **CONTEXTO APRENDIDO** para enriquecer sua resposta se for relevante.
        Lembre-se da REGRA CR칈TICA DE EMOTES
        '''

        # Montagem da Mensagem no formato da API Ollama
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"{long_term_context}\n\nQuery do Usu치rio: {query}"}
        ]
        
        payload = {
            "model": OLLAMA_MODEL,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": 0.7, 
                "num_ctx": 4096
            }
        }
        
        # Chamada  API do Ollama
        try:
            response = requests.post(f"{OLLAMA_URL}/api/chat", json=payload, timeout=30)
            response.raise_for_status() # Lan칞a exce칞칚o para c칩digos 4xx/5xx
            data = response.json()
            
            generated = data['message']['content'].strip()
            
        except requests.exceptions.RequestException as e:
            logging.error(f"[ERROR] Falha na comunica칞칚o com Ollama API: {e}")
            generated = "O portal est치 inst치vel. Eu n칚o consigo me comunicar. Sadge"

        # Limpeza Final e Salvamento de Mem칩ria
        generated = self._clean_response(generated)

        if generated and generated != "O portal est치 inst치vel. Eu n칚o consigo me comunicar. Sadge":
            # Salva a intera칞칚o query/response na mem칩ria de longo prazo (RAG)
            memory_mgr.save_user_memory(channel, author, query, generated)
            
            final_response = f"@{author}, {generated}"
            return final_response
        else:
            fallback = "Meow. O portal est치 com lag. Tente novamente! 游땾"
            final_fallback = f"@{author}, {fallback}"
            return final_fallback

    def _clean_response(self, generated):
        generated = generated.strip()
        
        generated = re.sub(r'\*([A-Za-z0-9]+)\*', r'\1', generated)

        generated = generated.replace("*EMOTE*:", "").strip()

        generated = re.sub(r'\[/INST\]', '', generated).strip()
        generated = re.sub(r'<\|eot_id\|>', '', generated).strip()

        return generated