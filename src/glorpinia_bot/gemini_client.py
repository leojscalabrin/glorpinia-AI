import os
import re
import logging
import google.generativeai as genai
from dotenv import load_dotenv

from .features.search import SearchTool

load_dotenv()

try:
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
except Exception as e:
    logging.error(f"Falha ao configurar a API do Google GenAI: {e}")
    raise

class GeminiClient:
    """
    Cliente para interagir com o modelo Gemini, com suporte a 
    m√∫ltiplos perfis (Lores de Canal), mem√≥ria RAG e busca na web.
    """
    def __init__(self, personality_profile):
        # O profile base (Glorpinia Padr√£o) fica guardado aqui
        self.base_profile = personality_profile
        
        # Dicion√°rio para guardar os modelos prontos de cada canal
        self.models_cache = {}
        
        self.cookie_system = None # Refer√™ncia injetada posteriormente

        self.generation_config = {
            "temperature": 0.7,
            "max_output_tokens": 1024, 
        }
        
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

        # Modelo Leve para An√°lises (Busca, Sumariza√ß√£o)
        self.analysis_model = genai.GenerativeModel(
            model_name="gemini-flash-latest",
            generation_config={"temperature": 0.1},
            safety_settings=self.safety_settings
        )

        # Inicializa a ferramenta de busca
        self.search_tool = SearchTool()
        
    def set_cookie_system(self, cookie_system):
        """Permite que o main.py injete o sistema de cookies aqui."""
        self.cookie_system = cookie_system

    def _get_model_for_channel(self, channel_name):
        """
        Recupera (ou cria) o modelo Gemini configurado especificamente para o canal.
        Verifica se existe um arquivo 'profile_{canal}.txt' para adicionar lore extra.
        """
        # Se j√° carregamos esse canal antes, retorna o modelo do cache (mem√≥ria RAM)
        if channel_name in self.models_cache:
            return self.models_cache[channel_name]

        # Se √© a primeira vez, vamos construir o modelo
        logging.info(f"[Gemini] Configurando personalidade para o canal: #{channel_name}...")
        
        # Come√ßa com a personalidade base da Glorpinia
        final_instruction = self.base_profile
        
        # Tenta carregar lore espec√≠fica do canal
        channel_profile_path = f"profile_{channel_name}.txt"
        
        if os.path.exists(channel_profile_path):
            try:
                with open(channel_profile_path, "r", encoding="utf-8") as f:
                    channel_lore = f.read()
                
                # FUS√ÉO: Adiciona a lore do canal ao final do system prompt
                final_instruction += f"\n\n[CONTEXTO ESPEC√çFICO DO CANAL #{channel_name}]\n{channel_lore}"
                logging.info(f"[Gemini] + Lore espec√≠fica de {channel_name} carregada com sucesso!")
            except Exception as e:
                logging.error(f"[Gemini] Erro ao ler {channel_profile_path}: {e}")
        else:
            logging.debug(f"[Gemini] Nenhum perfil espec√≠fico encontrado para {channel_name}. Usando base.")

        # Instancia o modelo para este canal
        new_model = genai.GenerativeModel(
            model_name="gemini-flash-latest", 
            generation_config=self.generation_config,
            safety_settings=self.safety_settings,
            system_instruction=final_instruction
        )

        # Salva no cache para n√£o ter que ler arquivo de novo
        self.models_cache[channel_name] = new_model
        return new_model

    def get_response(self, query, channel, author, memory_mgr=None, recent_history=None):
        """
        Gera uma resposta para o chat, usando o modelo espec√≠fico do canal.
        """
        # Limpa o input do usu√°rio
        clean_query = query.replace(f"@{author}", "").strip()
        
        # Formata o Hist√≥rico Recente (Context Window)
        chat_context_str = ""
        if recent_history:
            # Pega as √∫ltimas 15 mensagens para n√£o estourar tokens
            msgs = recent_history[-15:] 
            formatted_msgs = [f"- {m['author']}: {m['content']}" for m in msgs]
            chat_context_str = "**MENSAGENS RECENTES DO CHAT (Contexto Imediato):**\n" + "\n".join(formatted_msgs)
            
        # BUSCA NA WEB (Decis√£o Inteligente)
        web_context = ""
        try:
            if self._should_search(clean_query):
                # Usa a IA para limpar a query (ex: tira nicks, sauda√ß√µes)
                optimized_query = self._generate_search_query(clean_query)
                logging.info(f"[SearchTool] Query: '{clean_query}' -> '{optimized_query}'")

                # Faz a busca
                search_results = self.search_tool.perform_search(optimized_query)
                if search_results:
                    web_context = f"**CONTEXTO DA INTERNET (SOBRE '{optimized_query}'):**\n{search_results}"
        except Exception as e:
            logging.error(f"[Search Analysis Error] Falha: {e}")

        # MEM√ìRIA RAG
        memory_context = ""
        if memory_mgr:
            try:
                retrieved_memories = memory_mgr.search_memory(channel, clean_query)
                if retrieved_memories:
                    memory_context = f"**HIST√ìRICO RECENTE/RELEVANTE:**\n{retrieved_memories}"
            except Exception as e:
                logging.error(f"Erro ao buscar mem√≥ria: {e}")

        # Monta o Prompt Final
        prompt = f"""
        {chat_context_str}
        
        {memory_context}

        {web_context}

        **Mensagem do Usu√°rio:** {query}
        """

        try:
            # Pega o modelo correto para este canal (com ou sem lore extra)
            current_model = self._get_model_for_channel(channel)
            
            # Gera a resposta
            response = current_model.generate_content(prompt)
            generated = response.text.strip()
            
        except Exception as e:
            logging.error(f"[ERROR] Falha na comunica√ß√£o com a API Gemini: {e}")
            generated = "O portal est√° inst√°vel. Eu n√£o consigo me comunicar. Sadge"

        # Limpeza e Cookies
        generated = self._clean_response(generated)

        # Processa comandos de Cookie ocultos na resposta da IA
        if self.cookie_system:
            # Passamos o 'author' para saber se o cookie foi para ele ou para outro
            generated = self.cookie_system.process_ai_response(generated, current_user=author)

        # Salva na mem√≥ria e retorna
        if generated and "glorp-glorp" not in generated:
            if memory_mgr:
                memory_mgr.save_user_memory(channel, author, query, generated)
            
            final_response = f"@{author}, {generated}"
            return final_response
        else:
            return f"@{author}, Meow. O portal est√° com lag. Tente novamente! üò∏"

    def _should_search(self, query):
        """Decide se a query precisa de busca externa."""
        prompt = f"""
        Analise a mensagem abaixo e responda APENAS "SIM" ou "N√ÉO".
        O usu√°rio est√° perguntando sobre um fato objetivo, not√≠cia recente, defini√ß√£o t√©cnica, data hist√≥rica ou algo que requer conhecimento externo atualizado?
        Se for apenas papo furado, opini√£o, roleplay ou cumprimento, responda N√ÉO.

        Mensagem: {query}
        Resposta:
        """
        try:
            response = self.analysis_model.generate_content(prompt)
            decision = response.text.strip().upper()
            logging.info(f"[SearchTool] Decis√£o para '{query}': {decision}")
            return "SIM" in decision
        except:
            return False

    def _generate_search_query(self, user_message):
        """
        Usa a IA para transformar texto de chat em query de busca eficiente.
        """
        prompt = f"""
        Voc√™ √© um otimizador de buscas do Google.
        Transforme a mensagem do chat em uma query de pesquisa direta e simples.
        
        Regras:
        1. Remova sauda√ß√µes, men√ß√µes (@Nick) e emojis.
        2. Identifique o sujeito principal da d√∫vida.
        3. Se parecer um nome desconhecido, adicione 'quem √©' ou 'streamer'.
        
        Exemplos:
        Input: "@GlorpinIA quem √© o fabo?" -> Output: quem √© fabo streamer
        Input: "mano tu conhece o jogo elden ring?" -> Output: elden ring o que √©
        
        Input: {user_message}
        Output:
        """
        
        try:
            response = self.analysis_model.generate_content(
                prompt,
                generation_config={"temperature": 0.1}
            )
            return response.text.strip()
        except Exception as e:
            logging.error(f"Falha ao gerar query otimizada: {e}")
            return user_message

    def summarize_chat_topic(self, text_input: str) -> str:
        """
        Sumariza logs de chat ou transcri√ß√£o de √°udio.
        """
        if not text_input or len(text_input) < 5:
            return "nada em particular"

        prompt = f"""
        Identifique o t√≥pico MAIS INTERESSANTE ou ENGRA√áADO mencionado no texto abaixo.
        Seja breve (m√°x 5 palavras).
        
        Texto:
        {text_input}
        
        T√≥pico:
        """
        try:
            response = self.analysis_model.generate_content(prompt)
            topic = response.text.strip()
            return topic
        except:
            return "assuntos aleat√≥rios"

    def _clean_response(self, generated):
        """Limpa a resposta dos prefixos de prompt e metadados."""
        generated = generated.strip()
        # Remove blocos de contexto que a IA as vezes repete
        generated = re.sub(r'\*\*(CONTEXTO APRENDIDO|HIST√ìRICO RECENTE|CONTEXTO DA INTERNET)\*\*.*?\*RESPOSTA\*:?\s?', '', generated, flags=re.IGNORECASE | re.DOTALL).strip()
        # Remove timestamps ou prefixos de log
        generated = re.sub(r'^\[.*?\]\s*', '', generated)
        return generated