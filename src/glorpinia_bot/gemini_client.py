import os
import re
import logging
import google.generativeai as genai
from langchain_core.messages import HumanMessage, AIMessage
from .memory_manager import MemoryManager
from dotenv import load_dotenv

load_dotenv()

try:
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
except Exception as e:
    logging.error(f"Falha ao configurar a API do Google GenAI: {e}")
    raise

class GeminiClient:
    """
    Cliente para interagir com o modelo Gemini 1.5 Flash via API GenAI.
    """
    def __init__(self, personality_profile):
        self.personality_profile = personality_profile
        
        # Configura√ß√µes de gera√ß√£o (seguran√ßa e par√¢metros)
        self.generation_config = {
            "temperature": 0.7,
            "max_output_tokens": 1024,
        }
        
        # Configura√ß√µes de seguran√ßa (para permitir o roleplay)
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

        # Inicializa o modelo
        self.model = genai.GenerativeModel(
            model_name="gemini-flash-latest",
            generation_config=self.generation_config,
            safety_settings=self.safety_settings,
            system_instruction=self.personality_profile 
        )

    def get_response(self, query, channel, author, memory_mgr: 'MemoryManager'):
        """
        Gera uma resposta usando a API Gemini, injetando RAG como contexto.
        """
        memory_mgr.load_user_memory(channel, author)
        vectorstore = memory_mgr.vectorstore
        
        # Recupera√ß√£o de Contexto (RAG/Mem√≥ria de Longo Prazo)
        long_term_context = ""
        if vectorstore:
            try:
                retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
                docs = retriever.invoke(query)
                
                if docs:
                    long_term_context = "\n".join([doc.page_content for doc in docs])
                    # Formata o RAG para o prompt do Gemini
                    long_term_context = f"**CONTEXTO APRENDIDO (MEM√ìRIA GLORPINIA):** {long_term_context}"
            except Exception as e:
                logging.error(f"[RAG ERROR] Falha ao buscar contexto: {e}")

        # Montagem do Prompt Final (RAG + Query)
        prompt = f"""
        {long_term_context}

        **Query do Usu√°rio:** {query}
        """

        # chamada √† API do Gemini
        try:
            response = self.model.generate_content(prompt)

            # Verifica√ß√£o de seguran√ßa ANTES de tentar ler o .text
            if not response.parts:
                # A resposta foi bloqueada ou veio vazia
                finish_reason = "DESCONHECIDO"
                if response.candidates and response.candidates[0].finish_reason:
                    finish_reason = response.candidates[0].finish_reason.name # Pega o nome (ex: SAFETY)

                logging.error(f"[ERROR] A API Gemini n√£o retornou 'parts'. Finish Reason: {finish_reason}")
                
                if finish_reason == "SAFETY":
                    generated = "glorp [REDACTED]"
                else:
                    generated = f"Minhas anteninhas n√£o captaram nenhum sinal (Raz√£o: {finish_reason}). Sadge"
            else:
                # Se tudo estiver OK, agora sim podemos ler o texto
                generated = response.text.strip()

        except Exception as e:
            logging.error(f"[ERROR] Falha na comunica√ß√£o com a API Gemini: {e}")
            generated = "O portal est√° inst√°vel. Eu n√£o consigo me comunicar. Sadge"

        # Limpeza Final e Salvamento de Mem√≥ria
        generated = self._clean_response(generated)

        fallback = "Meow. O portal est√° com lag. Tente novamente! üò∏"
            
        # Verifica se o autor √© 'system'. Se for, n√£o adiciona @tag e n√£o salva na mem√≥ria.
        is_system_message = (author.lower() == "system")

        if generated:
            
            if is_system_message:
                # √â um 'comment' ou 'listen'. Retorna a resposta limpa.
                return generated
            else:
                # √â uma resposta a um usu√°rio. Salva na mem√≥ria e adiciona a tag.
                memory_mgr.save_user_memory(channel, author, query, generated)
                final_response = f"@{author}, {generated}"
                return final_response
        else:
            # L√≥gica de fallback
            if is_system_message:
                return fallback # Retorna o fallback limpo
            else:
                final_fallback = f"@{author}, {fallback}" # Retorna o fallback com tag
                return final_fallback
            
            return final_fallback
    
    def _clean_response(self, generated):
        
        generated = generated.strip()
        
        # Limpeza de lixo de RAG (se a mem√≥ria antiga ainda estiver suja)
        generated = re.sub(r'\*\*CONTEXTO APRENDIDO\*\*.*?\*RESPOSTA\*:?\s?', '', generated, flags=re.IGNORECASE).strip()
        generated = re.sub(r'(\*\*ESPACO DE EMOTES\*\*|\*\*ESPACO APRENDIDO\*\*):?.*?\s?', '', generated, flags=re.IGNORECASE).strip()
        
        return generated